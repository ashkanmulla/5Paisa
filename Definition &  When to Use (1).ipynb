{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cdda77c-00eb-4755-ba40-39b6493915e6",
   "metadata": {},
   "source": [
    "- My name is Ashkan Mulla. I have 7 years of experience as a lead data scientist in the financial services industry.\n",
    "- I completed my B.Tech from Mumbai University and my pursuing M.Tech in AI Engineering from Bits Pilani in 2023.\n",
    "- In my previous role at 5 Paisa Capital, I lead a 6 menbers team to develop GenAi,LLM, ML,GPT, VAE, and GANs and NLP and DL models.\n",
    "- Also collaborating with cross-functional teams including Data Engineers, product, BI, Operations, Sales, finance, and IT.\n",
    "- I have good expertise with NLP model architectures and algorithms like NLTK, BERT, Spacy etc for business problem.\n",
    "- I worked with different departments to integrate data science solutions into overall business strategies.\n",
    "- I engaged with stakeholders to understand their needs and effectively communicated data insights.\n",
    "- In my role, I train and validate machine learning models, including deep learning and statistical approaches.\n",
    "- I have practical experience in creating RESTful services.\n",
    "- I focus on understanding each use-case's complexity, performance, and reliability and understanding data privacy.\n",
    "- I ensure a comprehensive grasp of our applications and develop a deep understanding of the reasons behind our models and systems.\n",
    "- I helped manage multiple data science projects, ensuring they stayed on schedule and within budget.\n",
    "- I developed and deployed advanced AI models to solve complex business challenges.\n",
    "- I also designed and implemented NLP algorithms using LangChain to enhance text processing and understanding.\n",
    "- I build and improve models using CNNs, RNNs, and Transformers to make them work better\n",
    "- I handle projects all the way from researching ideas to deploying AI models, using Docker , AWS, API's to manage everything smoothly.\n",
    "- Also have good expertise to create proof-of-concept projects or prototypes and maintain Document and AI code and models.\n",
    "- I have Excellent communication skills for technical and non-technical audiences and i am Self-motivated and adaptable in dynamic environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321297c2-3a12-4620-a461-01696a6ff5f4",
   "metadata": {},
   "source": [
    "Over the past two years, I have developed expertise in generative AI, deep learning, and NLP, These are important skills in tech today. I have successfully led projects, including developing an NLP, Gen AI, Deep Learning and LLM models. With the increasing demand for AI skills in various industries, my knowledge provides a competitive edge. My advanced skills enhance productivity and drive innovative solutions, contributing to company growth. I am committed to continuous learning and staying updated with the latest advancements. For example, I implemented a deep learning algorithm that improved our product recommendation system, leading to a 15% increase in sales. These accomplishments demonstrate the value I bring to any organization.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d37408-1539-498b-8120-8bb56b2dabf6",
   "metadata": {},
   "source": [
    "#### **GEN AI PROJECT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04de36fc-4360-4052-82c1-c179fea5a123",
   "metadata": {},
   "source": [
    "- **Problem Statement & Objective**: Financial Industries find it difficult to review and understand clients' financial activities because it's a slow and manual process. The goal is to create system to automatically sort expenses, categorization, fraud detection, give financial trading advice, check loan eligibility, and keep track of financial health and risk assessments,quickly Sort Transactions,detect unusual activities,Provide Financial Insights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30de002-7f47-4b2d-be0a-12aaf13873b3",
   "metadata": {},
   "source": [
    "- **Approaches**:\n",
    "- (1) Data Gethering : we collect data in different format like PDF, CSV etc.\n",
    "- (2) Data Annotation with relevent categories like income, expense, investment and metadata like date, amount, description.\n",
    "- (3) Data Preprocessing : We use OCR to convert scanned or pdf statements into machine readable text.\n",
    "- (4) Data Cleaning: Steps include text normalization, removing special characters,Tokenization for Break down transaction descriptions into tokens (words) for analysis, Named Entity Recognition for Identify and classify key information such as merchant names, locations, and transaction types.\n",
    "- (5) Model Development: Fine tune a BERT model to classify transection into predefined categories based on their descriptions and context. \n",
    "- (6) Use GPT to generate synthetic training data, enhancing the model’s ability to handle varied and uncommon transaction descriptions. Create effective prompts for the GPT model to ensure it generates relevant and high-quality synthetic data.\n",
    "- (7) Training: Train the BERT model on labeled transaction data to classify transactions into categories such as groceries, utilities, entertainment, etc.\n",
    "- (8) Fine-Tuning: Fine-tune the model using a combination of real and synthetic data generated by GPT to improve accuracy and generalization.\n",
    "- (9) Model Evaluation: 1. Training and testing the data 2. Evaluation the model using precision, recall, f1 score and accuracy for classification tasks, and BLEU and ROUGE scores for text generation.\n",
    "- (10) Model Deployment: Using API integration and User Interface.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c601c-6283-4d85-943b-1395917cc98b",
   "metadata": {},
   "source": [
    "BERT Classification: \n",
    "- (1) Income : Salary, bonuses, investments, rentals, other gifts such as gifts ot freelance work\n",
    "- (2) Expenses: Rent, Groceries, Utilities, Transportation, entertainment, healthcare, education, insurance, debt repayment etc.\n",
    "- (3) Investments: Stocks, Bonds, Mutual Funds, real estate,etc.\n",
    "- (4) Transfers: Internal Transfer, external transfers, savings.\n",
    "- (5) Fees and charges : Bank Fees, Late Fees, Service charges etc.\n",
    "- (6) Refunds and reimbursements\n",
    "- (7) Taxes: tax related payments etc income tax.\n",
    "- (8) Implementation using BERT: (1) Data annotation (2) Fine Tuning BERT (3) Transection Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162bc14-4ad1-45bf-8205-086b5a4e2f0b",
   "metadata": {},
   "source": [
    "- Bias is when a model is too simple and misses important patterns (underfitting).\n",
    "- Variance is when a model is too complex and captures noise along with the actual pattern (overfiting).\n",
    "- The bias-variance trade-off is about finding the right balance: not too simple, not too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947ffa1-ad80-40d5-9ae3-33508bb15f93",
   "metadata": {},
   "source": [
    "- Bias: Bias is the error introduced by approximating a real-life problem with a simplified model. It measures how much the predictions of a model differ from the true values.\n",
    "\n",
    "- Variance: Variance measures the sensitivity of a model's predictions to changes in the training data. It quantifies how much the predictions for a given point vary between different realizations of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f9d453-091b-461e-95d7-30825c781426",
   "metadata": {},
   "source": [
    "- Overfiting is like memorizing the exact answers to practice questions but failing the actual test because you didn't understand the material.\n",
    "- Underfitting is like giving very vague answers that don’t actually address the practice questions or the actual test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c59752-0ba8-4c6c-b07f-142f8db9de89",
   "metadata": {},
   "source": [
    "- Regularization\n",
    "- Regularization is a technique used in machine learning to prevent models from overfiting the training data. Here's a simple explanation:\n",
    "\n",
    "- Explanation: Regularization is like adding a penalty or restriction to a model to keep it from becoming too complex. It helps the model generalize better to new, unseen data by discouraging it from fitting the training data too closely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29eb6d4-9548-42f7-8e50-e28d06f4ef22",
   "metadata": {},
   "source": [
    "**Reducing overfiting** involves simplifying the model, using cross-validation to tune parameters, and applying techniques like regularization to prevent it from memorizing the training data too closely. On the other hand, reducing underfitting requires using more complex models, improving feature engineering, and sometimes loosening regularization constraints to allow the model to capture more complex relationships in the data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e6c869-f273-4980-967b-e2115b497514",
   "metadata": {},
   "source": [
    "**What is Generative AI models**:  are computer systems that learn patterns from data to create new content or outputs, such as text, images, or music, resembling human-like creativity and problem-solving.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33666dbd-1fed-46e4-b19a-512cb3f73c5d",
   "metadata": {},
   "source": [
    "**LLM (Large Language Models)** are advanced AI systems trained on vast amounts of text data to understand and generate human-like language, enabling tasks like translation, summarization, and conversation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4e03fc-a317-455a-b0b8-7113771235da",
   "metadata": {},
   "source": [
    "Bias-Variance Trade-Off\n",
    "The bias-variance trade-off is the balance between these two types of errors. A model with:\n",
    "\n",
    "High Bias and Low Variance: Underfits the data, meaning it's too simple and doesn't capture the underlying patterns (e.g., a straight line trying to fit a complex curve).\n",
    "Low Bias and High Variance: Overfits the data, meaning it's too complex and captures noise along with the actual pattern (e.g., a squiggly line that fits every point in the training data perfectly but fails on new data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a961c6d5",
   "metadata": {},
   "source": [
    "<h1 style=\"color:purple\" > 1. Measure of Central Tendency </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37388f64",
   "metadata": {},
   "source": [
    "\n",
    "- Mean: Average; used for central tendency.\n",
    "- Median: Middle value; used for robustness to outliers.\n",
    "- Mode: Most frequent value; used for identifying commonality.\n",
    "- Weighted Mean: Average with weights; used for importance assignment.\n",
    "- Trimmed Mean: Central tendency with outliers removed; used for outlier resistance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e61e156",
   "metadata": {},
   "source": [
    "<h1 style=\"color:purple\" >2. Measure of Central Dispersion </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c859e221",
   "metadata": {},
   "source": [
    "\n",
    "- Range: Difference between max and min; used for data spread.\n",
    "- Variance: Average squared deviation from mean; used for data variability.\n",
    "- Standard Deviation: Square root of variance; used for spread in data units.\n",
    "- Coefficient of Variation: SD as a percentage of the mean; used for relative variability.\n",
    "\n",
    "\n",
    "- Quantile: Divides a dataset into intervals; used for data distribution analysis.\n",
    "\n",
    "- Percentile: Value below which a given percentage of observations fall; used for ranking data relative to others.\n",
    "- 5 Number Summary: Minimum, 25th percentile, median, 75th percentile, maximum; used for summarizing data distribution.\n",
    "- Box Plot: Visual representation of the 5 number summary; used for identifying outliers and distribution.\n",
    "- Scatter Plot: Graphical display of two variables' relationship; used for visualizing relationships between variables.\n",
    "- Covariance: Measure of how two variables change together; used for assessing linear relationship between variables.\n",
    "- Correlation: Measure of strength and direction of the linear relationship; used for understanding association between variables.\n",
    "- Causation: Relationship where one event is influenced by another; used for establishing cause-and-effect relationships.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f23335",
   "metadata": {},
   "source": [
    "<h1 style=\"color:purple\" >Univariate Analysis - Single Column Observation </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7c1edc",
   "metadata": {},
   "source": [
    "### Graph for Numerical Data Analysis\n",
    "\n",
    "- Histogram: Visual representation of the distribution of a numerical variable; used for understanding data distribution.\n",
    "\n",
    "- Scatter Plot: Graphical display of two variables' relationship; used for visualizing relationships between variables.\n",
    "\n",
    "- Box Plot (Box and Whisker Plot): Visual representation of the distribution of a numerical variable, including quartiles and outliers; used for identifying outliers and distribution. \n",
    "- Kernel Density Plot (KDE): Smoothed version of a histogram, showing the probability density function of a numerical variable; used for visualizing data distribution.\n",
    "\n",
    "- Line & Strip Plot: Plotting points or lines to show trends or relationships between variables; used for visualizing trends over time or across categories.\n",
    "\n",
    "- Word Cloud: Visual representation of the frequency of words in a text; used for identifying common or important words in text data.\n",
    "\n",
    "- Empirical Cumulative Density Function (ECDF): Graphical representation of the cumulative distribution function of a numerical variable; used for visualizing the distribution of data and comparing distributions.\n",
    "\n",
    "- Swarm Plot: Plotting points along a categorical axis with a small amount of random jitter; used for visualizing the distribution of categorical data points and avoiding overlap.\n",
    "- IQR (Interquartile Range): Measure of statistical dispersion, representing the range between the 25th and 75th percentiles of a dataset.\n",
    "- When to use: To assess the spread and variability of the middle 50% of a dataset, and to identify outliers or extreme values.\n",
    "\n",
    "\n",
    "### Graph for Categorical Data Analysis\n",
    "\n",
    "- Bar Chart: Visual representation of categorical data using rectangular bars; used for comparing categories.\n",
    "- Pie Chart: Circular chart divided into sectors to illustrate proportion; used for showing parts of a whole.\n",
    "- Count Plot: Plotting the count of occurrences of categorical variables; used for visualizing frequency distributions.\n",
    "- Donut Chart: Circular chart similar to a pie chart, with a hole in the center; used for showing parts of a whole with emphasis on the overall composition.\n",
    "- Word Cloud: Visual representation of the frequency of words in a text; used for identifying common or important words in text data.\n",
    "- Tree Map: Hierarchical representation of data using nested rectangles; used for displaying hierarchical data structures.\n",
    "- Stacked Bar Chart: Bar chart with bars stacked on top of each other; used for comparing the contribution of different categories to a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5d76df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e23c4b33",
   "metadata": {},
   "source": [
    "**Skewness**: Measure of the asymmetry of the probability distribution of a real-valued random variable.\n",
    "- The range of skewness values is typically from -3 to +3 for a symmetric distribution.\n",
    "- Negative skewness values indicate left skewness, while positive skewness values indicate right skewness.\n",
    "- Extreme values beyond the range of -3 to +3 indicate highly skewed distributions.\n",
    "- When to use: To understand the direction and degree of asymmetry in a dataset.\n",
    "\n",
    "\n",
    "\n",
    "**Kurtosis**: Measure of the \"tailedness\" of the probability distribution of a real-valued random variable.\n",
    "- The range of kurtosis values is typically from -3 to +∞.\n",
    "- A kurtosis value of 0 indicates the same tail behavior as a normal distribution.\n",
    "- Positive kurtosis values indicate heavier tails, while negative values indicate lighter tails.\n",
    "- Extreme positive values indicate heavy tails and a sharp peak, while extreme negative values indicate light tails and a flat peak.\n",
    "- When to use: To understand the shape of the tails and the peakness of the distribution in a dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad4e2ec",
   "metadata": {},
   "source": [
    "<h1 style=\"color:purple\" >Probability Distribution </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99423cc2",
   "metadata": {},
   "source": [
    "- Random Variables: Variables whose values depend on outcomes of a random phenomenon.\n",
    "\n",
    "- When to use: To represent uncertain or stochastic quantities in a probabilistic model.\n",
    "- Probability Distribution: Mathematical function that describes the likelihood of obtaining different outcomes of a random variable.\n",
    "\n",
    "- When to use: To model the behavior of random variables and understand their probabilities of occurrence.\n",
    "- Probability Distribution Function: Function that describes the probability distribution of a continuous random variable.\n",
    "\n",
    "- When to use: To calculate probabilities of specific outcomes or ranges of values for continuous random variables.\n",
    "- Probability Mass Function: Function that describes the probability distribution of a discrete random variable.\n",
    "\n",
    "- When to use: To calculate probabilities of specific outcomes or ranges of values for discrete random variables.\n",
    "- Cumulative Distribution Function: Function that describes the probability that a random variable takes on a value less than or equal to a given value.\n",
    "\n",
    "- When to use: To calculate probabilities of values falling below a certain threshold or to assess the overall distribution of a random variable.\n",
    "- Probability Density Function: Function that describes the probability distribution of a continuous random variable in terms of its density.\n",
    "\n",
    "- When to use: To represent the probability distribution of continuous random variables and calculate probabilities of specific intervals.\n",
    "- Density Estimation: Process of estimating the underlying probability density function of a random variable from observed data.\n",
    "\n",
    "- When to use: To model the distribution of data and make inferences about its underlying structure.\n",
    "- Parametric Density Estimation: Density estimation method that assumes a specific parametric form for the probability density function.\n",
    "\n",
    "- When to use: When the underlying distribution of data is known or can be assumed to follow a particular parametric form.\n",
    "- Non-Parametric Density Estimation: Density estimation method that does not make assumptions about the form of the probability density function.\n",
    "\n",
    "- When to use: When the underlying distribution of data is unknown or cannot be assumed to follow a specific parametric form.\n",
    "- Kernel Density Estimate (KDE): Non-parametric method for estimating the probability density function of a random variable based on observed data.\n",
    "\n",
    "- When to use: To estimate the underlying probability density function of data when no assumptions about its form are made.\n",
    "- Cumulative Distribution Function (CDF) of PDF: Function that describes the cumulative probability distribution of a continuous random variable.\n",
    "\n",
    "- When to use: To calculate probabilities of values falling below a certain threshold or to assess the overall distribution of a continuous random variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ebf32",
   "metadata": {},
   "source": [
    "### Normal Distribution and Non Gaussian Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa41ea1b",
   "metadata": {},
   "source": [
    "- Normal Distribution: A symmetric probability distribution where most values cluster around the mean, forming a bell-shaped curve.\n",
    "\n",
    "- When to use: To model the distribution of continuous data that approximate a normal distribution in real-world phenomena.\n",
    "- Standard Normal Variate: A variable transformed to have a mean of 0 and a standard deviation of 1 under the standard normal distribution.\n",
    "\n",
    "- When to use: To standardize data for comparison and analysis in statistical tests and modeling.\n",
    "\n",
    "- Q-Q Plot (Quantile-Quantile Plot): Graphical method to compare the quantiles of a dataset against those of a theoretical distribution.\n",
    "\n",
    "- When to use: To assess if a dataset follows a specific theoretical distribution, such as the normal distribution.\n",
    "- Uniform Distribution: Probability distribution where all outcomes have equal probability; characterized by a constant probability density function.\n",
    "\n",
    "- When to use: To model situations where outcomes are equally likely, such as in random number generation or when dealing with equally probable events.\n",
    "Log Normal Distribution: Probability distribution of a random variable whose logarithm is normally distributed.\n",
    "\n",
    "- When to use: To model data that are positively skewed and have a wide range of values, such as incomes, population sizes, and stock prices.\n",
    "- Pareto Distribution: Probability distribution where a small number of values account for the majority of occurrences; characterized by a heavy right tail.\n",
    "\n",
    "- When to use: To model phenomena where a small number of factors contribute to the majority of outcomes, such as wealth distribution, income distribution, and popularity of internet content.\n",
    "- Transformation: Process of converting data from one form to another, such as applying logarithmic or exponential transformations.\n",
    "\n",
    "- When to use: To stabilize variance, make data more normally distributed, or linearize relationships between variables in real data analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b7269",
   "metadata": {},
   "source": [
    "### Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec93a7e7",
   "metadata": {},
   "source": [
    "- Central Limit Theorem: Statistical theory stating that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the shape of the population distribution.\n",
    "\n",
    "- When to use: To understand the behavior of sample means and assess the reliability of statistical inference in real data analysis.\n",
    "- Bernoulli's Distribution: Probability distribution of a random variable with two possible outcomes, usually denoted as success and failure.\n",
    "\n",
    "- When to use: To model experiments with binary outcomes, such as coin flips, success/failure trials, or yes/no questions.\n",
    "- Binomial Distribution: Probability distribution of the number of successes in a fixed number of independent Bernoulli trials.\n",
    "\n",
    "- When to use: To model the number of successes in a series of independent binary experiments, such as the number of heads in a series of coin flips or the number of successes in a fixed number of trials.\n",
    "- Sampling Distribution: Distribution of a sample statistic, such as the sample mean or sample proportion, across different samples from the same population.\n",
    "\n",
    "- When to use: To make inferences about the population parameters based on sample statistics in real data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae3143f",
   "metadata": {},
   "source": [
    "### Confidence Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80cbcf",
   "metadata": {},
   "source": [
    "- Point Estimate: Single value used to estimate a population parameter, typically derived from sample data.\n",
    "\n",
    "- When to use: To provide a best guess of a population parameter based on sample data.\n",
    "- Confidence Interval: Range of values around a point estimate that is likely to contain the true population parameter with a certain level of confidence.\n",
    "\n",
    "- When to use: To quantify the uncertainty associated with a point estimate and provide a range of plausible values for the population parameter.\n",
    "- Confidence Interval when Sigma Known (Z-procedure): Calculation of the confidence interval using the standard normal distribution, typically applicable when the population standard deviation (sigma) is known.\n",
    "\n",
    "- When to use: When the population standard deviation is known and sample size is large, allowing for the use of the normal distribution.\n",
    "- Confidence Interval when Sigma is Not Known (T-procedure): Calculation of the confidence interval using the student's t-distribution, typically applicable when the population standard deviation (sigma) is unknown.\n",
    "\n",
    "- When to use: When the population standard deviation is unknown or when the sample size is small, requiring the use of the t-distribution for more accurate estimation.\n",
    "- Student's T Distribution: Probability distribution used for calculating confidence intervals and hypothesis testing when the population standard deviation is unknown and sample size is small.\n",
    "\n",
    "- When to use: When working with small sample sizes or when the population standard deviation is unknown in real data analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f787fb2",
   "metadata": {},
   "source": [
    "### Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe98e09",
   "metadata": {},
   "source": [
    "- What is Hypothesis testing: A statistical hypothesis test is a method of statistical inference used to decide\n",
    "whether the data at hand sufficiently support a particular hypothesis. Hypothesis\n",
    "testing allows us to make probabilistic statements about population parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5ac780",
   "metadata": {},
   "source": [
    "Null hypothesis (H0):\n",
    "\n",
    "- In simple terms, the null hypothesis is a statement that assumes there is no significant\n",
    "effect or relationship between the variables being studied. It serves as the starting point\n",
    "for hypothesis testing and represents the status quo or the assumption of no effect until\n",
    "proven otherwise. The purpose of hypothesis testing is to gather evidence (data) to either\n",
    "reject or fail to reject the null hypothesis in favour of the alternative hypothesis, which\n",
    "claims there is a significant effect or relationship.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d359ae",
   "metadata": {},
   "source": [
    "Alternative hypothesis (H1 or Ha):\n",
    "\n",
    "- The alternative hypothesis, is a statement that contradicts the null hypothesis and claims\n",
    "there is a significant effect or relationship between the variables being studied. It\n",
    "represents the research hypothesis or the claim that the researcher wants to support\n",
    "through statistical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e538b012",
   "metadata": {},
   "source": [
    "Important Points\n",
    "- How to decide what will be Null hypothesis and what will be Alternate\n",
    "\n",
    "- Hypothesis(Typically the Null hypothesis says nothing new is happening)\n",
    "•\n",
    "- We try to gather evidence to reject the null hypothesis•\n",
    "- It's important to note that failing to reject the null hypothesis doesn't necessarily mean\n",
    "that the null hypothesis is true; it just means that there isn't enough evidence to support\n",
    "the alternative hypothesis.\n",
    "•\n",
    "- Hypothesis tests are similar to jury trials, in a sense. In a jury trial, \n",
    "- H0 is similar to the not-guilty verdict,\n",
    "- and Ha is the guilty verdict. \n",
    "- You assume in a jury trial that the defendant isn’t guilty unless the\n",
    "prosecution can show beyond a reasonable doubt that he or she is guilty. \n",
    "- If the jury says the evidence is\n",
    "beyond a reasonable doubt, they reject H0, not guilty, in favour of Ha , guilty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c0bb4a",
   "metadata": {},
   "source": [
    "### What is the use of Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e288c3",
   "metadata": {},
   "source": [
    "- Hypothesis testing is like playing detective with data. It helps us figure out if what we observe in a sample is just random chance or if there's something meaningful going on in the larger population. By setting up hypotheses and collecting evidence, we can decide whether our ideas about the world are supported by the data or if we need to rethink them.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5314f226",
   "metadata": {},
   "source": [
    "### Steps involved in Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d3ce4d",
   "metadata": {},
   "source": [
    "Rejection Region Approach\n",
    "\n",
    "- 1.Formulate a Null and Alternate hypothesis\n",
    "- 2.Select a significance level(This is the probability of rejecting the null hypothesis when it is actually true, usually set at 0.05 or 0.01)\n",
    "- 3.Check assumptions (example distribution)\n",
    "- 4.Decide which test is appropriate(Z-test, T-test, Chi-square test, ANOVA)\n",
    "- 5.State the relevant test statistic\n",
    "- 6.Conduct the test\n",
    "- 7.Reject or not reject the Null Hypothesis.\n",
    "- 8.Interpret the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1859a67",
   "metadata": {},
   "source": [
    "### What is Z - test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f102827",
   "metadata": {},
   "source": [
    "- The Z-test is like a detective tool used in statistics to figure out if something we observe in a sample is really important or just a fluke. We use it when we have a good idea about how data should behave based on a big group (population) and want to check if a smaller group (sample) behaves differently in a significant way. It helps us decide if what we're seeing is real or just random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ff268",
   "metadata": {},
   "source": [
    "### What is Student t- test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f3cc6",
   "metadata": {},
   "source": [
    "The Student's t-distribution, or t-test, is a statistical tool used to compare the means of two groups and determine if they are significantly different from each other, even when the sample sizes are small or when the population standard deviation is unknown.\n",
    "\n",
    "- We use the t-test when we want to compare two groups and see if the differences we observe are real or just due to random chance. It's like a magnifying glass for small samples, helping us detect meaningful differences between groups despite limited data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de32e032",
   "metadata": {},
   "source": [
    "#### Difference Between Z- test and Student t- test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b50c6f",
   "metadata": {},
   "source": [
    "Z-test\n",
    "- Used when population standard deviation is known\n",
    "- Typically applied with large sample sizes\n",
    "- Requires knowledge of population parameters\n",
    "- Assumes normal distribution of data\n",
    "- Critical values are based on standard normal distribution\n",
    "\n",
    "\n",
    "t-test\n",
    "- Used when population standard deviation is unknown or sample size is small\n",
    "- Typically applied with small sample sizes\n",
    "- Does not require knowledge of population parameters\n",
    "- Can handle non-normal distributions\n",
    "- Critical values are based on t-distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d56f8c0",
   "metadata": {},
   "source": [
    "Rejection Region\n",
    "\n",
    "- Significance level- denoted as α (alpha), is a predetermined threshold used in hypothesis\n",
    "testing to determine whether the null hypothesis should be rejected or not. It represents the\n",
    "probability of rejecting the null hypothesis when it is actually true, also known as Type 1 error.\n",
    "\n",
    "\n",
    "- The critical region is the region of values that corresponds to the rejection of the null\n",
    "hypothesis at some chosen probability level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37cf4a1",
   "metadata": {},
   "source": [
    "### Type 1 vs Type 2 Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b9ef5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In hypothesis testing, there are two types of errors that can\n",
    "occur when making a decision about the null hypothesis: Type\n",
    "I error and Type II error.\n",
    "\n",
    "\n",
    "\n",
    "Type-I (False Positive) error occurs when the sample results,\n",
    "lead to the rejection of the null hypothesis when it is in fact\n",
    "true.\n",
    "\n",
    "\n",
    "In other words, it's the mistake of finding a significant effect or\n",
    "relationship when there is none. The probability of committing\n",
    "a Type I error is denoted by α (alpha), which is also known as\n",
    "the significance level. By choosing a significance level,\n",
    "researchers can control the risk of making a Type I error.\n",
    "\n",
    "\n",
    "\n",
    "Type-II (False Negative) error occurs when based on the\n",
    "sample results, the null hypothesis is not rejected when it is in\n",
    "fact false.\n",
    "\n",
    "\n",
    "This means that the researcher fails to detect a significant\n",
    "effect or relationship when one actually exists. The probability\n",
    "of committing a Type II error is denoted by β (beta).\n",
    "\n",
    "\n",
    "Trade-off between Type 1 and Type 2 errors   (alpha = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99b29ae",
   "metadata": {},
   "source": [
    "### One sided vs two sided test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42812d4a",
   "metadata": {},
   "source": [
    "One-sided (one-tailed) test: \n",
    "    \n",
    "    \n",
    "A one-sided test is used when the researcher is interested in\n",
    "testing the effect in a specific direction (either greater than or less than the value specified in\n",
    "the null hypothesis). The alternative hypothesis in a one-sided test contains an inequality\n",
    "(either \">\" or \"<\").\n",
    "\n",
    "\n",
    "\n",
    "Example: A researcher wants to test whether a new medication increases the average\n",
    "recovery rate compared to the existing medication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6632d36",
   "metadata": {},
   "source": [
    "Two-sided (two-tailed) test: \n",
    "    \n",
    "    \n",
    "A two-sided test is used when the researcher is interested in\n",
    "testing the effect in both directions (i.e., whether the value specified in the null hypothesis is\n",
    "different, either greater or lesser). The alternative hypothesis in a two-sided test contains a\n",
    "\"not equal to\" sign (≠).\n",
    "\n",
    "\n",
    "\n",
    "Example: A researcher wants to test whether a new medication has a different average\n",
    "recovery rate compared to the existing medication.\n",
    "\n",
    "\n",
    "The main difference between them lies in the directionality of the alternative hypothesis and\n",
    "how the significance level is distributed in the critical regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb5ed4",
   "metadata": {},
   "source": [
    "One-tailed test (one-sided):\n",
    "    \n",
    "    \n",
    "Advantages:\n",
    "More powerful: One-tailed tests are generally more powerful than two-tailed tests, as the\n",
    "entire significance level (α) is allocated to one tail of the distribution. This means that the\n",
    "test is more likely to detect an effect in the specified direction, assuming the effect exists.\n",
    "\n",
    "\n",
    "\n",
    "Directional hypothesis: One-tailed tests are appropriate when there is a strong theoretical\n",
    "or practical reason to test for an effect in a specific direction.\n",
    "\n",
    "\n",
    "Disadvantages:\n",
    "Missed effects: One-tailed tests can miss effects in the opposite direction of the specified\n",
    "alternative hypothesis. If an effect exists in the opposite direction, the test will not be\n",
    "able to detect it, which could lead to incorrect conclusions.\n",
    "\n",
    "\n",
    "Increased risk of Type I error: One-tailed tests can be more prone to Type I errors if the\n",
    "effect is actually in the opposite direction than the one specified in the alternative\n",
    "hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dfffcc",
   "metadata": {},
   "source": [
    "### Where can be Hypothesis Testing Applied?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c52f33",
   "metadata": {},
   "source": [
    "Testing the effectiveness of interventions or treatments: Hypothesis testing can be used to\n",
    "determine whether a new drug, therapy, or educational intervention has a significant effect\n",
    "compared to a control group or an existing treatment.\n",
    "\n",
    "\n",
    "\n",
    "1.Comparing means or proportions: Hypothesis testing can be used to compare means or\n",
    "proportions between two or more groups to determine if there's a significant difference.\n",
    "This can be applied to compare average customer satisfaction scores, conversion rates, or\n",
    "employee performance across different groups.\n",
    "\n",
    "\n",
    "2.Analysing relationships between variables: Hypothesis testing can be used to evaluate the\n",
    "association between variables, such as the correlation between age and income or the\n",
    "relationship between advertising spend and sales.\n",
    "\n",
    "\n",
    "3.Evaluating the goodness of fit: Hypothesis testing can help assess if a particular theoretical\n",
    "distribution (e.g., normal, binomial, or Poisson) is a good fit for the observed data.\n",
    "\n",
    "\n",
    "4.Testing the independence of categorical variables: Hypothesis testing can be used to\n",
    "determine if two categorical variables are independent or if there's a significant association\n",
    "between them. For example, it can be used to test if there's a relationship between the\n",
    "type of product and the likelihood of it being returned by a customer.\n",
    "\n",
    "\n",
    "5.A/B testing: In marketing, product development, and website design, hypothesis testing is\n",
    "often used to compare the performance of two different versions (A and B) to determine\n",
    "which one is more effective in terms of conversion rates, user engagement, or other\n",
    "metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0f5bfb",
   "metadata": {},
   "source": [
    "### Hypothesis Testing ML Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33768bd",
   "metadata": {},
   "source": [
    "1. Model comparison: Hypothesis testing can be used to compare the performance of\n",
    "different machine learning models or algorithms on a given dataset. For example, you can\n",
    "use a paired t-test to compare the accuracy or error rate of two models on multiple cross-\n",
    "validation folds to determine if one model performs significantly better than the other.\n",
    "\n",
    "\n",
    "2. Feature selection: Hypothesis testing can help identify which features are significantly\n",
    "related to the target variable or contribute meaningfully to the model's performance. For\n",
    "example, you can use a t-test, chi-square test, or ANOVA to test the relationship between\n",
    "individual features and the target variable. Features with significant relationships can be\n",
    "selected for building the model, while non-significant features may be excluded.\n",
    "\n",
    "\n",
    "3. Hyperparameter tuning: Hypothesis testing can be used to evaluate the performance of a\n",
    "model trained with different hyperparameter settings. By comparing the performance of\n",
    "models with different hyperparameters, you can determine if one set of hyperparameters\n",
    "leads to significantly better performance.\n",
    "\n",
    "\n",
    "4. Assessing model assumptions: In some cases, machine learning models rely on certain\n",
    "statistical assumptions, such as linearity or normality of residuals in linear regression.\n",
    "Hypothesis testing can help assess whether these assumptions are met, allowing you to\n",
    "determine if the model is appropriate for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f93c6d2",
   "metadata": {},
   "source": [
    "### P-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1490a628",
   "metadata": {},
   "source": [
    "- P-value is the probability of getting a sample as or more extreme(having more evidence\n",
    "against H0) than our own sample given the Null Hypothesis(H0) is true.\n",
    "\n",
    "\n",
    "In simple words p-value is a measure of the strength of the evidence against the Null\n",
    "Hypothesis that is provided by our sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32abffad",
   "metadata": {},
   "source": [
    "### Interpreting p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e9ca38",
   "metadata": {},
   "source": [
    "With significance value  alpha = 0.05\n",
    "\n",
    "Without significance value\n",
    "1. Very small p-values (e.g., p < 0.01) indicate strong evidence against the null hypothesis,\n",
    "suggesting that the observed effect or difference is unlikely to have occurred by chance\n",
    "alone.\n",
    "\n",
    "2. Small p-values (e.g., 0.01 ≤ p < 0.05) indicate moderate evidence against the null\n",
    "hypothesis, suggesting that the observed effect or difference is less likely to have\n",
    "occurred by chance alone.\n",
    "\n",
    "3. Large p-values (e.g., 0.05 ≤ p < 0.1) indicate weak evidence against the null hypothesis,\n",
    "suggesting that the observed effect or difference might have occurred by chance alone,\n",
    "but there is still some level of uncertainty.\n",
    "\n",
    "4. Very large p-values (e.g., p ≥ 0.1) indicate weak or no evidence against the null\n",
    "hypothesis, suggesting that the observed effect or difference is likely to have occurred by\n",
    "chance alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a79fdf",
   "metadata": {},
   "source": [
    "## T-tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe77149f",
   "metadata": {},
   "source": [
    "- A t-test is a statistical test used in hypothesis testing to compare the means of two samples or to compare a sample mean to a known population mean. The t-test is based on the t-\n",
    "distribution, which is used when the population standard deviation is unknown and the sample size is small.\n",
    "\n",
    "\n",
    "There are three main types of t-tests:\n",
    "    \n",
    "    \n",
    "- One-sample t-test: The one-sample t-test is used to compare the mean of a single sample to a\n",
    "known population mean. The null hypothesis states that there is no significant difference\n",
    "between the sample mean and the population mean, while the alternative hypothesis states\n",
    "that there is a significant difference.\n",
    "\n",
    "\n",
    "- Independent two-sample t-test: The independent two-sample t-test is used to compare the\n",
    "means of two independent samples. The null hypothesis states that there is no significant\n",
    "difference between the means of the two samples, while the alternative hypothesis states that\n",
    "there is a significant difference.\n",
    "\n",
    "\n",
    "- Paired t-test (dependent two-sample t-test): The paired t-test is used to compare the means of\n",
    "two samples that are dependent or paired, such as pre-test and post-test scores for the same\n",
    "group of subjects or measurements taken on the same subjects under two different\n",
    "conditions. The null hypothesis states that there is no significant difference between the\n",
    "means of the paired differences, while the alternative hypothesis states that there is a\n",
    "significant difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58c927",
   "metadata": {},
   "source": [
    "#### 1. Single Sample t-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b947df9",
   "metadata": {},
   "source": [
    "A one-sample t-test checks whether a sample mean differs from the population mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db851bce",
   "metadata": {},
   "source": [
    "- Assumptions for a single sample t-test\n",
    "\n",
    "1. Normality - Population from which the sample is drawn is normally distributed\n",
    "Independence - The observations in the sample must be independent, which means that\n",
    "the value of one observation should not influence the value of another observation.\n",
    "2.\n",
    "Random Sampling - The sample must be a random and representative subset of the\n",
    "population.\n",
    "3.\n",
    "4. Unknown population std - The population std is not known."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1547f0a8",
   "metadata": {},
   "source": [
    "#### 2. Independent 2 sample t-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c7bc5",
   "metadata": {},
   "source": [
    "An independent two-sample t-test, also known as an unpaired t-test, is a statistical method\n",
    "used to compare the means of two independent groups to determine if there is a significant\n",
    "difference between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a29c71d",
   "metadata": {},
   "source": [
    "- Assumptions for the test:\n",
    "\n",
    "\n",
    "1. Independence of observations: The two samples must be independent, meaning there is\n",
    "no relationship between the observations in one group and the observations in the other\n",
    "group. The subjects in the two groups should be selected randomly and independently.\n",
    "\n",
    "\n",
    "2. Normality: The data in each of the two groups should be approximately normally\n",
    "distributed. The t-test is considered robust to mild violations of normality, especially\n",
    "when the sample sizes are large (typically n ≥ 30) and the sample sizes of the two groups are similar. If the data is highly skewed or has substantial outliers, consider using a non-\n",
    "parametric test, such as the Mann-Whitney U test.\n",
    "\n",
    "\n",
    "3. Equal variances (Homoscedasticity): The variances of the two populations should be\n",
    "approximately equal. This assumption can be checked using F-test for equality of\n",
    "variances. If this assumption is not met, you can use Welch's t-test, which does not\n",
    "require equal variances.\n",
    "\n",
    "\n",
    "4. Random sampling: The data should be collected using a random sampling method from\n",
    "the respective populations. This ensures that the sample is representative of the\n",
    "population and reduces the risk of selection bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba55269c",
   "metadata": {},
   "source": [
    "#### 3. Paired 2 sample t-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7493af91",
   "metadata": {},
   "source": [
    "A paired two-sample t-test, also known as a dependent or paired-samples t-test, is a statistical\n",
    "test used to compare the means of two related or dependent groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b1ddf2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Common scenarios where a paired two-sample t-test is used include:\n",
    "\n",
    "    \n",
    "1. Before-and-after studies: Comparing the performance of a group before and after an\n",
    "intervention or treatment.\n",
    "\n",
    "2. Matched or correlated groups: Comparing the performance of two groups that are\n",
    "matched or correlated in some way, such as siblings or pairs of individuals with similar\n",
    "characteristics.\n",
    "\n",
    "Assumptions\n",
    "\n",
    "\n",
    "1. Paired observations: The two sets of observations must be related or paired in some way,\n",
    "such as before-and-after measurements on the same subjects or observations from\n",
    "matched or correlated groups.\n",
    "\n",
    "\n",
    "2. Normality: The differences between the paired observations should be approximately\n",
    "normally distributed. This assumption can be checked using graphical methods (e.g.,\n",
    "histograms, Q-Q plots) or statistical tests for normality (e.g., Shapiro-Wilk test). Note that\n",
    "the t-test is generally robust to moderate violations of this assumption when the sample\n",
    "size is large.\n",
    "\n",
    "\n",
    "3. Independence of pairs: Each pair of observations should be independent of other pairs. In\n",
    "other words, the outcome of one pair should not affect the outcome of another pair. This\n",
    "assumption is generally satisfied by appropriate study design and random sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b871f37",
   "metadata": {},
   "source": [
    "## Chi Square Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ae3fa8",
   "metadata": {},
   "source": [
    "The Chi-Square distribution, also written as χ2 distribution, is a continuous\n",
    "probability distribution that is widely used in statistical hypothesis testing,\n",
    "particularly in the context of goodness-of-fit tests and tests for independence in\n",
    "contingency tables. It arises when the sum of the squares of independent\n",
    "standard normal random variables follows this distribution.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The Chi-Square distribution has a single parameter, the degrees of freedom (df),\n",
    "which influences the shape and spread of the distribution. The degrees of\n",
    "freedom are typically associated with the number of independent variables or\n",
    "constraints in a statistical problem.\n",
    "\n",
    "\n",
    "- Some key properties of the Chi-Square distribution are:\n",
    "\n",
    "\n",
    "1. It is a continuous distribution, defined for non-negative values.\n",
    "It is positively skewed, with the degree of skewness decreasing as the\n",
    "degrees of freedom increase. \n",
    "\n",
    "2. The mean of the Chi-Square distribution is equal to its degrees of\n",
    "freedom, and its variance is equal to twice the degrees of freedom.\n",
    "\n",
    "3. As the degrees of freedom increase, the Chi-Square distribution\n",
    "approaches the normal distribution in shape.\n",
    "\n",
    "\n",
    "The Chi-Square distribution is used in various statistical tests, such as the Chi-\n",
    "Square goodness-of-fit test, which evaluates whether an observed frequency distribution fits an expected theoretical distribution, and the Chi-Square test for\n",
    "independence, which checks the association between categorical variables in a\n",
    "contingency table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29868a67",
   "metadata": {},
   "source": [
    "## Chi Square Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9140425b",
   "metadata": {},
   "source": [
    "The Chi-Square test is a statistical hypothesis test used to determine if there is a significant\n",
    "association between categorical variables or if an observed distribution of categorical data\n",
    "differs from an expected theoretical distribution. It is based on the Chi-Square (χ2) distribution,\n",
    "and it is commonly applied in two main scenarios:\n",
    "    \n",
    "    \n",
    "    \n",
    "- 1. Chi-Square Goodness-of-Fit Test: This test is used to determine if the observed\n",
    "distribution of a single categorical variable matches an expected theoretical distribution.\n",
    "It is often applied to check if the data follows a specific probability distribution, such as\n",
    "the uniform or binomial distribution.\n",
    "\n",
    "\n",
    "- 2. Chi-Square Test for Independence (Chi-Square Test for Association): This test is used to\n",
    "determine whether there is a significant association between two categorical variables in\n",
    "a sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f44daa",
   "metadata": {},
   "source": [
    "### Goodness of Fit Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25c20a8",
   "metadata": {},
   "source": [
    "The Chi-Square Goodness-of-Fit test is a statistical hypothesis test used to\n",
    "determine if the observed distribution of a single categorical variable\n",
    "matches an expected theoretical distribution. It helps to evaluate\n",
    "whether the data follows a specific probability distribution, such as\n",
    "uniform, binomial, or Poisson distribution, among others. This test is\n",
    "particularly useful when you want to assess if the sample data is\n",
    "consistent with an assumed distribution or if there are significant\n",
    "deviations from the expected pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac50ba2",
   "metadata": {},
   "source": [
    "Assumptions\n",
    "\n",
    "1. Independence: \n",
    "\n",
    "2. Categorical data:\n",
    "\n",
    "3. Expected frequency: \n",
    "\n",
    "4. Fixed distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b55e3",
   "metadata": {},
   "source": [
    "The Chi-Square Goodness-of-Fit test is a non-parametric test. Non-\n",
    "parametric tests do not assume that the data comes from a specific probability distribution or make any assumptions about population parameters like the mean or standard deviation.\n",
    "\n",
    "\n",
    "\n",
    "In the Chi-Square Goodness-of-Fit test, we compare the observed frequencies of the categorical data to the expected frequencies based on a hypothesized distribution. The test doesn't rely on any assumptions about the underlying distribution's parameters. Instead, it focuses on comparing observed counts to expected counts, making it a non-\n",
    "parametric test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30debd9",
   "metadata": {},
   "source": [
    "### Test for Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3a7943",
   "metadata": {},
   "source": [
    "1. The Chi-Square test for independence, also known as the Chi-Square test for association, is a\n",
    "statistical test used to determine whether there is a significant association between two\n",
    "categorical variables in a sample. It helps to identify if the occurrence of one variable is\n",
    "dependent on the occurrence of the other variable, or if they are independent of each other.\n",
    "\n",
    "\n",
    "2. The test is based on comparing the observed frequencies in a contingency table (a table that\n",
    "displays the frequency distribution of the variables) with the frequencies that would be\n",
    "expected under the assumption of independence between the two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61556e98",
   "metadata": {},
   "source": [
    "Assumptions\n",
    "1. Independence of observations: \n",
    "2. Categorical variables: \n",
    "3. Adequate sample size: \n",
    "4. Fixed marginal totals: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e13592",
   "metadata": {},
   "source": [
    "### Applications in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4808decc",
   "metadata": {},
   "source": [
    "1. Feature selection: Chi-Square test can be used as a filter-based feature selection method to\n",
    "rank and select the most relevant categorical features in a dataset. By measuring the\n",
    "association between each categorical feature and the target variable, you can eliminate\n",
    "irrelevant or redundant features, which can help improve the performance and efficiency\n",
    "of machine learning models.\n",
    "\n",
    "\n",
    "2. Evaluation of classification models: For multi-class classification problems, the Chi-Square\n",
    "test can be used to compare the observed and expected class frequencies in the confusion\n",
    "matrix. This can help assess the goodness of fit of the classification model, indicating how\n",
    "well the model's predictions align with the actual class distributions.\n",
    "\n",
    "\n",
    "3. Analysing relationships between categorical features: In exploratory data analysis, the Chi-\n",
    "Square test for independence can be applied to identify relationships between pairs of categorical features. Understanding these relationships can help inform feature\n",
    "engineering and provide insights into the underlying structure of the data.\n",
    "\n",
    "\n",
    "4. Discretization of continuous variables: When converting continuous variables into\n",
    "categorical variables (binning), the Chi-Square test can be used to determine the optimal\n",
    "number of bins or intervals that best represent the relationship between the continuous\n",
    "variable and the target variable.\n",
    "\n",
    "\n",
    "5. Variable selection in decision trees: Some decision tree algorithms, such as the CHAID (Chi-\n",
    "squared Automatic Interaction Detection) algorithm, use the Chi-Square test to determine the most significant splitting variables at each node in the tree. This helps construct more\n",
    "effective and interpretable decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0956dc",
   "metadata": {},
   "source": [
    "## AVOVA TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5f1d40",
   "metadata": {},
   "source": [
    "### F - Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c679472",
   "metadata": {},
   "source": [
    "1. Continuous probability distribution: The F-distribution is a continuous\n",
    "probability distribution used in statistical hypothesis testing and\n",
    "analysis of variance (ANOVA).\n",
    "\n",
    "\n",
    "2. Fisher-Snedecor distribution: It is also known as the Fisher-Snedecor\n",
    "distribution, named after Ronald Fisher and George Snedecor, two\n",
    "prominent statisticians.\n",
    "\n",
    "\n",
    "3. Degrees of freedom: The F-distribution is defined by two parameters -\n",
    "the degrees of freedom for the numerator (df1) and the degrees of\n",
    "freedom for the denominator (df2).\n",
    "\n",
    "\n",
    "4. Positively skewed and bounded: The shape of the F-distribution is\n",
    "positively skewed, with its left bound at zero. The distribution's shape\n",
    "depends on the values of the degrees of freedom.\n",
    "\n",
    "\n",
    "5. esting equality of variances: The F-distribution is commonly used to\n",
    "test hypotheses about the equality of two variances in different\n",
    "samples or populations.\n",
    "\n",
    "\n",
    "6. Comparing statistical models: The F-distribution is also used to compare\n",
    "the fit of different statistical models, particularly in the context of\n",
    "ANOVA.\n",
    "\n",
    "\n",
    "7. F-statistic: The F-statistic is calculated by dividing the ratio of two\n",
    "sample variances or mean squares from an ANOVA table. This value is\n",
    "then compared to critical values from the F-distribution to determine\n",
    "statistical significance.\n",
    "\n",
    "8. Applications: The F-distribution is widely used in various fields of\n",
    "research, including psychology, education, economics, and the natural\n",
    "and social sciences, for hypothesis testing and model comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14310a0f",
   "metadata": {},
   "source": [
    "### One way ANOVA test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c146b9a",
   "metadata": {},
   "source": [
    "One-way ANOVA (Analysis of Variance) is a statistical method used to compare the means of\n",
    "three or more independent groups to determine if there are any significant differences\n",
    "between them. It is an extension of the t-test, which is used for comparing the means of two\n",
    "independent groups. The term \"one-way\" refers to the fact that there is only one independent\n",
    "variable (factor) with multiple levels (groups) in this analysis.\n",
    "\n",
    "\\\n",
    "The primary purpose of one-way ANOVA is to test the null hypothesis that all the group means\n",
    "are equal. The alternative hypothesis is that at least one group mean is significantly different\n",
    "from the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495aeae8",
   "metadata": {},
   "source": [
    "It's important to note that one-way ANOVA only determines if there is a significant difference\n",
    "between the group means; it does not identify which specific groups have significant\n",
    "differences. To determine which pairs of groups are significantly different, post-hoc tests, such\n",
    "as Tukey's HSD or Bonferroni, are conducted after a significant ANOVA result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff286f",
   "metadata": {},
   "source": [
    "**Assumptions**\n",
    "1. Independence:\n",
    "\n",
    "2. Normality:\n",
    "\n",
    "3. Homogeneity of variances: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9571e3",
   "metadata": {},
   "source": [
    "### Why t-test is not used for more than 3 categories?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8447e15a",
   "metadata": {},
   "source": [
    "1. Increased Type I error: \n",
    "\n",
    "2. Difficulty in interpreting results: \n",
    "3. Inefficiency: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695db77c",
   "metadata": {},
   "source": [
    "### Applications in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edd63e0",
   "metadata": {},
   "source": [
    "1. Hyperparameter tuning: When selecting the best hyperparameters for a machine learning\n",
    "model, one-way ANOVA can be used to compare the performance of models with different\n",
    "hyperparameter settings. By treating each hyperparameter setting as a group, you can\n",
    "perform one-way ANOVA to determine if there are any significant differences in\n",
    "performance across the various settings.\n",
    "\n",
    "\n",
    "2. Feature selection: One-way ANOVA can be used as a univariate feature selection method to\n",
    "identify features that are significantly associated with the target variable, especially when\n",
    "the target variable is categorical with more than two levels. In this context, the one-way\n",
    "ANOVA is performed for each feature, and features with low p-values are considered to be\n",
    "more relevant for prediction.\n",
    "\n",
    "\n",
    "3. Algorithm comparison: When comparing the performance of different machine learning\n",
    "algorithms, one-way ANOVA can be used to determine if there are any significant\n",
    "differences in their performance metrics (e.g., accuracy, F1 score, etc.) across multiple runs\n",
    "or cross-validation folds. This can help you decide which algorithm is the most suitable for a\n",
    "specific problem.\n",
    "\n",
    "\n",
    "4. Model stability assessment: One-way ANOVA can be used to assess the stability of a\n",
    "machine learning model by comparing its performance across different random seeds or\n",
    "initializations. If the model's performance varies significantly between different\n",
    "initializations, it may indicate that the model is unstable or highly sensitive to the choice of\n",
    "initial conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152f150f",
   "metadata": {},
   "source": [
    "## What is Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f9385c",
   "metadata": {},
   "source": [
    "Multicollinearity is a statistical phenomenon that occurs when two or more independent\n",
    "variables in a multiple regression model are highly correlated. In other words, these variables\n",
    "exhibit a strong linear relationship, making it difficult to isolate the individual effects of each\n",
    "variable on the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d0fe08",
   "metadata": {},
   "source": [
    "### What exactly happens in Multicollinearity(Mathematically?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c742cb0",
   "metadata": {},
   "source": [
    "When multicollinearity is present in a model, it can lead to several issues, including:\n",
    "    \n",
    "    \n",
    "1. Difficulty in identifying the most important predictors: Due to the high correlation between\n",
    "independent variables, it becomes challenging to determine which variable has the most\n",
    "significant impact on the dependent variable.\n",
    "\n",
    "\n",
    "2. Inflated standard errors: Multicollinearity can lead to larger standard errors for the\n",
    "regression coefficients, which decreases the statistical power and can make it challenging\n",
    "to determine the true relationship between the independent and dependent variables.\n",
    "\n",
    "\n",
    "3. Unstable and unreliable estimates: The regression coefficients become sensitive to small\n",
    "changes in the data, making it difficult to interpret the results accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676b14ad",
   "metadata": {},
   "source": [
    "**Perfect Multicollinearity**\n",
    "\n",
    "- Perfect multicollinearity occurs when one independent variable in a multiple regression model\n",
    "is an exact linear combination of one or more other independent variables. In other words,\n",
    "there is an exact linear relationship between the independent variables, making it impossible\n",
    "to uniquely estimate the individual effects of each variable on the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc312c6",
   "metadata": {},
   "source": [
    "**Types of Multicollinearity**\n",
    "\n",
    "- Structural multicollinearity: Structural multicollinearity arises due to the way in which the\n",
    "variables are defined or the model is constructed. It occurs when one independent variable\n",
    "is created as a linear combination of other independent variables or when the model\n",
    "includes interaction terms or higher-order terms (such as polynomial terms) without proper\n",
    "scaling or centering.\n",
    "\n",
    "\n",
    "- Data-driven multicollinearity: Data-driven multicollinearity occurs when the independent\n",
    "variables in the dataset are highly correlated due to the specific data being analysed. In this\n",
    "case, the high correlation between the variables is not a result of the way the variables are\n",
    "defined or the model is constructed but rather due to the observed data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c7035d",
   "metadata": {},
   "source": [
    "**How to Detect Multicollinearity**\n",
    "\n",
    "1. Correlation is a measure of the linear relationship between two variables, and it is commonly\n",
    "used to identify multicollinearity in multiple linear regression models. Multicollinearity occurs\n",
    "when two or more predictor variables in the model are highly correlated, making it difficult to\n",
    "determine their individual contributions to the output variable.\n",
    "\n",
    "\n",
    "2. Variance Inflation Factor (VIF) is a metric used to quantify the severity of multicollinearity in a\n",
    "multiple linear regression model. It measures the extent to which the variance of an estimated\n",
    "regression coefficient is increased due to multicollinearity.\n",
    "\n",
    "3. Condition number is a diagnostic measure used toassess the stability and potential numerical issues in a multiple linear regression model. Itprovides an indication of the severity of multicollinearity by examining the sensitivity of the linear regression to small changes in the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68e85d5",
   "metadata": {},
   "source": [
    "**How to remove multicollinearity**\n",
    "\n",
    "1. Collect more data: In some cases, multicollinearity might be a result of a limited sample\n",
    "size. Collecting more data, if possible, can help reduce multicollinearity and improve the\n",
    "stability of the model.\n",
    "\n",
    "\n",
    "2. Remove one of the highly correlated variables: If two or more independent variables are\n",
    "highly correlated, consider removing one of them from the model. This step can help\n",
    "eliminate redundancy in the model and reduce multicollinearity. Choose the variable to\n",
    "remove based on domain knowledge, variable importance, or the one with the highest VIF.\n",
    "\n",
    "\n",
    "3. Combine correlated variables: If correlated independent variables represent similar\n",
    "information, consider combining them into a single variable. This combination can be done\n",
    "by averaging, summing, or using other mathematical operations, depending on the context\n",
    "and the nature of the variables.\n",
    "\n",
    "\n",
    "4. Use partial least squares regression (PLS): PLS is a technique that combines features of both\n",
    "principal component analysis and multiple regression. It identifies linear combinations of\n",
    "the predictor variables (called latent variables) that have the highest covariance with the\n",
    "response variable, reducing multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ad9cb",
   "metadata": {},
   "source": [
    "## MODEL EVALUATION REGRESSION MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1224b5d1",
   "metadata": {},
   "source": [
    "**Mean Absolute Error (MAE):**\n",
    "\n",
    "MAE measures the average absolute difference between the predicted values and the actual values. It is calculated as the average of the absolute differences between predicted and actual values. MAE is easy to interpret as it represents the average magnitude of errors in the predictions.\n",
    "\n",
    "\n",
    "**Mean Squared Error (MSE):**\n",
    "\n",
    "MSE measures the average of the squares of the errors between predicted values and actual values. It is calculated as the average of the squared differences between predicted and actual values. MSE gives more weight to large errors compared to MAE.\n",
    "\n",
    "\n",
    "**Root Mean Squared Error (RMSE):**\n",
    "\n",
    "RMSE is the square root of the mean squared error. It provides a measure of the average magnitude of error in the predicted values, in the same units as the target variable. RMSE is widely used and gives a more interpretable measure of error compared to MSE.\n",
    "\n",
    "\n",
    "**R-squared (R²):**\n",
    "\n",
    "R-squared is a statistical measure that represents the proportion of variance explained by the independent variables in the model. It ranges from 0 to 1, where 1 indicates that the model explains all the variability of the response data around its mean. R-squared is often used as a goodness-of-fit measure for regression models.\n",
    "\n",
    "\n",
    "**Adjusted R-squared:**\n",
    "\n",
    "Adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in the model. It penalizes the addition of unnecessary variables that do not improve the model's performance. Adjusted R-squared tends to be lower than R-squared when additional predictors do not significantly improve the model's fit.\n",
    "\n",
    "\n",
    "**Mean Absolute Percentage Error (MAPE):**\n",
    "\n",
    "MAPE measures the average percentage difference between the predicted and actual values. It is calculated as the average of the absolute percentage differences between predicted and actual values, expressed as a percentage. MAPE is useful for assessing the accuracy of forecasts, especially in cases where the magnitude of errors is significant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f5255",
   "metadata": {},
   "source": [
    "## MODEL EVALUATION CLASSIFICATION MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2575f330",
   "metadata": {},
   "source": [
    "**True Positives (TP):**\n",
    "\n",
    "- True Positives are the cases where the model correctly predicts the positive class.\n",
    "- For example, in a binary classification task where the positive class represents \"has disease,\" true positives would be the instances where the model correctly identifies individuals with the disease.\n",
    "\n",
    "\n",
    "**True Negatives (TN):**\n",
    "\n",
    "- True Negatives are the cases where the model correctly predicts the negative class.\n",
    "- Using the same example, true negatives would represent instances where the model correctly identifies individuals without the disease.\n",
    "\n",
    "\n",
    "**False Positives (FP) (Type I Error):**\n",
    "    \n",
    "- False Positives are the cases where the model incorrectly predicts the positive class when it is actually negative.\n",
    "- Continuing with the disease example, false positives would occur when the model predicts someone has the disease when they don't.\n",
    "\n",
    "\n",
    "**False Negatives (FN) (Type II Error):**\n",
    "    \n",
    "- False Negatives are the cases where the model incorrectly predicts the negative class when it is actually positive.\n",
    "- In the disease example, false negatives would be instances where the model fails to identify someone who actually has the disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d431774",
   "metadata": {},
   "source": [
    "**Accuracy:** The proportion of correct predictions (TP + TN) over the total number of predictions.\n",
    "    \n",
    "**Precision:** The proportion of true positive predictions over all positive predictions (TP / (TP + FP)).\n",
    "    \n",
    "**Recall (Sensitivity):** The proportion of true positive predictions over all actual positive instances (TP / (TP + FN)).\n",
    "    \n",
    "**F1 Score:** The harmonic mean of precision and recall, which provides a balance between the two metrics.\n",
    "    \n",
    "**Specificity:** The proportion of true negative predictions over all actual negative instances (TN / (TN + FP)).\n",
    "    \n",
    "**False Positive Rate (FPR):** The proportion of false positives over all actual negative instances (FP / (FP + TN)).\n",
    "    \n",
    "**False Negative Rate (FNR):** The proportion of false negatives over all actual positive instances (FN / (FN + TP))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987226b5",
   "metadata": {},
   "source": [
    "**Confusion Matrix:**\n",
    "A confusion matrix is a table used to evaluate the performance of a classification model. It presents a summary of the predictions made by the model against the actual values in the dataset. The confusion matrix typically consists of four terms: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
    "    \n",
    "    \n",
    "**Precision:**\n",
    "Precision is a measure of the accuracy of positive predictions made by the model. It is calculated as the ratio of true positive predictions to the total number of positive predictions (TP / (TP + FP)). Precision indicates the proportion of positive identifications that were actually correct.\n",
    "\n",
    "\n",
    "**Recall (Sensitivity):**\n",
    "Recall, also known as sensitivity, measures the model's ability to correctly identify positive instances from the entire population of actual positives. It is calculated as the ratio of true positive predictions to the total number of actual positive instances (TP / (TP + FN)).\n",
    "\n",
    "\n",
    "**F1 Score:**\n",
    "The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall, especially when the class distribution is imbalanced. The formula for calculating the F1 score is: 2 * ((Precision * Recall) / (Precision + Recall)).\n",
    "    \n",
    "    \n",
    "**ROC (Receiver Operating Characteristic) Curve:**\n",
    "The ROC curve is a graphical representation of the performance of a binary classification model across different thresholds. It plots the true positive rate (TPR), also known as recall or sensitivity, against the false positive rate (FPR). Each point on the ROC curve represents a different threshold for classifying positive instances. A model with good predictive performance will have an ROC curve that is closer to the top-left corner of the plot.\n",
    "\n",
    "\n",
    "**AUC (Area Under the ROC Curve):**\n",
    "AUC is a scalar value that represents the area under the ROC curve. It provides a single metric to assess the overall performance of a binary classification model. AUC ranges from 0 to 1, where a value of 1 indicates perfect classification, and a value of 0.5 suggests random classification. Higher AUC values indicate better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0136ed5c",
   "metadata": {},
   "source": [
    "**AUC - ROC CURVE**\n",
    "\n",
    "\n",
    "The AUC-ROC measures the entire two-dimensional area underneath the entire\n",
    "ROC curve from (0,0) to (1,1). AUC provides an aggregate measure of\n",
    "performance across all possible classification thresholds.\n",
    "\n",
    "\n",
    "- An AUC of 1 indicates that the model has perfect discrimination: it correctly classifies all positive and negative instances.\n",
    "\n",
    "- An AUC of 0.5 suggests the model has no discrimination ability: it is as good as random guessing.\n",
    "\n",
    "- An AUC of 0 indicates that the model is perfectly wrong: it classifies all\n",
    "positive instances as negative and all negative instances as positive.\n",
    "\n",
    "\n",
    "In practice, AUC values usually fall between 0.5 (random) and 1 (perfect), with\n",
    "higher values indicating better classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8949bff",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b49f61c",
   "metadata": {},
   "source": [
    "1. determines how well a machine learning model performs for a given dataset. \n",
    "\n",
    "\n",
    "\n",
    "2. It calculates the difference between the expected value and predicted value and represents it as a single real number.\n",
    "\n",
    "\n",
    "3. Cost function is a measure of how wrong the model is in estimating the relationship between X(input) and Y(output) Parameter\n",
    "\n",
    "\n",
    "4. Gradient Descent is an optimization algorithm which is used for optimizing the cost function or error in the model.\n",
    "\n",
    "\n",
    "5. Types of Cost Function 1. Regression Cost Function. 2. Binary Classification cost Functions. 3. Multi-class Classification Cost Function.\n",
    "\n",
    "\n",
    "6. he cost function is calculated as the error based on the distance, such as:  1. Error= Actual Output-Predicted output  \n",
    "        \n",
    "        \n",
    "7. Binary Cost Function: Classification models are used to make predictions of categorical variables, such as predictions for 0 or 1,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec276d5b-5b68-48bd-bd9b-52ac3037e91a",
   "metadata": {},
   "source": [
    "## What is Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffd3669-26b1-4f3d-b699-1bd48361fab3",
   "metadata": {},
   "source": [
    "- **Linear regression is a simple statistical method used to understand the relationship between two continuous variables. It helps in predicting the value of a dependent variable (Y) based on the value of an independent variable (X)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272741da-9a55-4970-85cf-4c4055b70429",
   "metadata": {},
   "source": [
    "Key Concepts\n",
    "1. The Line of Best Fit:\n",
    "\n",
    "The goal of linear regression is to find the best-fitting straight line through the data points. This line is called the \"line of best fit\"\n",
    "The equation of this line is usually written as\n",
    "\n",
    "\n",
    "y__\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑋\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X, where:\n",
    "𝑌\n",
    "Y is the predicted value.\n",
    "𝛽\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    "  is the intercept (the value of Y when X is 0).\n",
    "𝛽\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    "  is the slope (the change in Y for a one-unit change in X).\n",
    "- 2. Assumptions of Linear Regression:\n",
    "\n",
    "- Linearity: The relationship between the independent and dependent variable should be linear.\n",
    "- Independence: The observations should be independent of each other.\n",
    "- Homoscedasticity: The residuals (differences between observed and predicted values) should have constant variance.\n",
    "- Normality: The residuals should be approximately normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7bb1c3-50a0-47e4-a582-09884770ac45",
   "metadata": {},
   "source": [
    "The goal of linear regression is to find the best-fitting straight line through the data points. This line is called the \"line of best fit\"\n",
    "The equation of this line is usually written as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38a3825-9642-442a-a955-9058773682b6",
   "metadata": {},
   "source": [
    "- 1. Multiple Linear Regression:\n",
    "\n",
    "- If you have more than one independent variable, you can use multiple linear regression. The equation extends to \n",
    "y-\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑋\n",
    "1\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "𝑋\n",
    "2\n",
    "+\n",
    ".\n",
    ".\n",
    ".\n",
    "+\n",
    "𝛽\n",
    "𝑛\n",
    "𝑋\n",
    "𝑛\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    "​\n",
    " +...+β \n",
    "n\n",
    "​\n",
    " X \n",
    "n\n",
    "​\n",
    " .\n",
    "2. R-squared (R²):\n",
    "\n",
    "- This statistic indicates how well the independent variable(s) explain the variability of the dependent variable. It ranges from 0 to 1, with 1 indicating perfect prediction.\n",
    "3. P-Value:\n",
    "\n",
    "- This helps determine the significance of the independent variable(s). A low p-value (typically < 0.05) indicates that the relationship is statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292158d-d83e-49bf-bca7-e4c21b17e334",
   "metadata": {},
   "source": [
    "## What is Logistic Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48138d9-6dc9-452c-9fdd-4cd900a74d9d",
   "metadata": {},
   "source": [
    "\n",
    "**Logistic regression is a statistical method used to model the probability of a binary outcome (a result that can be one of two possible values). It predicts the likelihood that a given input belongs to a certain class.** \n",
    "\n",
    "Key Points\n",
    "Binary Outcome:\n",
    "\n",
    "Logistic regression is used when the dependent variable is binary (e.g., yes/no, 0/1, true/false).\n",
    "The Logistic Function:\n",
    "\n",
    "The logistic regression model is based on the logistic function, which outputs probabilities between 0 and 1: \n",
    "𝑃\n",
    "(\n",
    "𝑌\n",
    "\n",
    "1\n",
    "∣\n",
    "𝑋\n",
    ")\n",
    "\n",
    "1\n",
    "1\n",
    "+\n",
    "𝑒\n",
    "−\n",
    "(\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑋\n",
    ")\n",
    "P(Y=1∣X)= \n",
    "1+e \n",
    "−(β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X)\n",
    " \n",
    "1\n",
    "​\n",
    " \n",
    "𝑃\n",
    "(\n",
    "𝑌\n",
    "\n",
    "1\n",
    "∣\n",
    "𝑋\n",
    ")\n",
    "P(Y=1∣X) is the probability that the outcome is 1 given the input \n",
    "𝑋\n",
    "X.\n",
    "𝛽\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    "  is the intercept.\n",
    "𝛽\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    "  is the coefficient for the independent variable \n",
    "𝑋\n",
    "X.\n",
    "𝑒\n",
    "e is the base of the natural logarithm.\n",
    "Log Odds:\n",
    "\n",
    "The logistic function models the log odds of the probability, ensuring that the predicted probabilities are always between 0 and 1.\n",
    "\n",
    "**Assumptions of Logistic Regression**\n",
    "- 1. Binary Dependent Variable: The outcome should be binary.\n",
    "- 2. Linearity of Independent Variables and Log Odds: The relationship between the independent variables and the log odds should be linear.\n",
    "- 3. Independence: Observations should be independent of each other.\n",
    "- 4. No Multicollinearity: Independent variables should not be highly correlated with each other.\n",
    "- 5. Large Sample Size: Logistic regression generally requires a large sample size to provide reliable estimates.\n",
    "     \n",
    "When to Use Logistic Regression\n",
    "When you want to predict the probability of a binary outcome.\n",
    "When the dependent variable is binary, and you need to classify the outcome.\n",
    "To understand the impact of independent variables on the probability of the dependent event occurring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab1c39f-ac14-496f-adf2-8b1ff738ece6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab37a3-ec12-463e-94c1-08f0b0a2de15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cbff53-761c-4c2d-9711-a191857307a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cd76788-9c85-497f-931f-98d03966099f",
   "metadata": {},
   "source": [
    "- R Strip method:\n",
    "The rstrip() method in Python removes any trailing characters (characters at the end of a string). By default, it removes whitespace characters.\n",
    "\n",
    "- example = \"Hello, World!   \"\n",
    "print(example.rstrip())  # Output: \"Hello, World!\"\n",
    "\n",
    "- Elite Function:\n",
    "There is no commonly known concept called \"Elite Function\" in Python or general programming terminology. It might be a specific term used in a particular context or domain.\n",
    "\n",
    "- Lambda Function:\n",
    "A lambda function in Python is a small anonymous function defined with the lambda keyword. It can have any number of arguments but only one expression.\n",
    "\n",
    "add = lambda x, y: x + y\n",
    "print(add(2, 3))  # Output: 5\n",
    "Filter Function:\n",
    "\n",
    "- The filter() function in Python constructs an iterator from elements of an iterable for which a function returns true. It filters elements based on a condition.\n",
    "\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "even_numbers = filter(lambda x: x % 2 == 0, numbers)\n",
    "print(list(even_numbers))  # Output: [2, 4]\n",
    "\n",
    "- Pickling:\n",
    "Pickling in Python is the process of converting a Python object into a byte stream to store it in a file or database, or transmit it over a network. This is done using the pickle module.\n",
    "import pickle\n",
    "data = {'a': 1, 'b': 2}\n",
    "with open('data.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "- To load the data back\n",
    "with open('data.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "print(loaded_data)  # Output: {'a': 1, 'b': 2}\n",
    "\n",
    "- Max No. of Vowels:\n",
    "To find the maximum number of vowels in a string, iterate through the string and count occurrences of the vowels (a, e, i, o, u).\n",
    "python\n",
    "Copy code\n",
    "def max_vowels(s):\n",
    "    vowels = 'aeiouAEIOU'\n",
    "    return sum(1 for char in s if char in vowels)\n",
    "\n",
    "example = \"Hello, World!\"\n",
    "print(max_vowels(example))  # Output: 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3366b1a7-0b6f-443c-8c6c-cf209587f889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
